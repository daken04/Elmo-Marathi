{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Downtask 2: Wikipedia Section-title Prediction"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Marathi"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:28:58.136760Z","iopub.status.busy":"2024-05-08T17:28:58.136143Z","iopub.status.idle":"2024-05-08T17:29:26.039779Z","shell.execute_reply":"2024-05-08T17:29:26.038829Z","shell.execute_reply.started":"2024-05-08T17:28:58.136725Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting indic-nlp-library\n","  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n","Collecting sphinx-argparse (from indic-nlp-library)\n","  Downloading sphinx_argparse-0.4.0-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.2.4)\n","Collecting morfessor (from indic-nlp-library)\n","  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (2.1.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.4)\n","Collecting sphinx>=1.2.0 (from sphinx-argparse->indic-nlp-library)\n","  Downloading sphinx-7.3.7-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n","Collecting sphinxcontrib-applehelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\n","Collecting sphinxcontrib-devhelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\n","Collecting sphinxcontrib-jsmath (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\n","Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\n","Collecting sphinxcontrib-qthelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\n","Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n","Requirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.17.2)\n","Requirement already satisfied: docutils<0.22,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.21.1)\n","Requirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n","Requirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n","Collecting alabaster~=0.7.14 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n","Collecting imagesize>=1.3 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n","  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n","Requirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n","Requirement already satisfied: tomli>=2 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n","Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n","Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n","Downloading sphinx-7.3.7-py3-none-any.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n","Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n","Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n","Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, alabaster, sphinx, sphinx-argparse, indic-nlp-library\n","Successfully installed alabaster-0.7.16 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-7.3.7 sphinx-argparse-0.4.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\n","Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (15.0.2)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["!pip install indic-nlp-library\n","!pip install pandas pyarrow"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:29:26.042293Z","iopub.status.busy":"2024-05-08T17:29:26.041902Z","iopub.status.idle":"2024-05-08T17:29:27.730850Z","shell.execute_reply":"2024-05-08T17:29:27.729913Z","shell.execute_reply.started":"2024-05-08T17:29:26.042255Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA Available: True\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","cuda_available = torch.cuda.is_available()\n","print(\"CUDA Available:\", cuda_available)\n","device = torch.device(\"cuda\" if cuda_available else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:29:27.732295Z","iopub.status.busy":"2024-05-08T17:29:27.731938Z","iopub.status.idle":"2024-05-08T17:29:27.744314Z","shell.execute_reply":"2024-05-08T17:29:27.743454Z","shell.execute_reply.started":"2024-05-08T17:29:27.732271Z"},"trusted":true},"outputs":[],"source":["class ELMoLanguageModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(ELMoLanguageModel, self).__init__()\n","        self.forward_lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n","        self.forward_lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n","        self.backward_lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n","        self.backward_lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n","\n","        self.forward_pred = nn.Linear(hidden_dim, vocab_size)\n","        self.backward_pred = nn.Linear(hidden_dim, vocab_size)\n","        self.gamma = nn.Parameter(torch.ones(3))  \n","        self.freeze_parameters()\n","        \n","    def freeze_parameters(self):\n","        for name, param in self.named_parameters():\n","            if 'gamma' not in name:\n","                param.requires_grad = False\n","\n","    def forward(self, x):\n","        \n","        forward_out1, _ = self.forward_lstm1(x)\n","        forward_out2, _ = self.forward_lstm2(forward_out1)\n","\n","        # Backward LM\n","        reversed_embeddings = torch.flip(x, [1])\n","        backward_out1, _ = self.backward_lstm1(reversed_embeddings)\n","        backward_out2, _ = self.backward_lstm2(backward_out1)\n","\n","        backward_out1 = torch.flip(backward_out1, [1])\n","        backward_out2 = torch.flip(backward_out2, [1])\n","\n","        forward_predictions = self.forward_pred(forward_out2[:, -1, :])\n","        backward_predictions = self.backward_pred(backward_out2[:, 0, :])\n","\n","        combined_embeddings = self.gamma[0] * x + self.gamma[1] * torch.cat((forward_out1, backward_out1), dim=-1) + self.gamma[2] * torch.cat((forward_out2, backward_out2), dim=-1)\n","\n","        return forward_predictions, backward_predictions, combined_embeddings\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:29:27.747307Z","iopub.status.busy":"2024-05-08T17:29:27.746855Z","iopub.status.idle":"2024-05-08T17:29:32.688992Z","shell.execute_reply":"2024-05-08T17:29:32.688212Z","shell.execute_reply.started":"2024-05-08T17:29:27.747275Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","\n","def load_model_and_mappings(model_path, mappings_path):\n","    with open(mappings_path, 'r', encoding='utf-8') as f:\n","        mappings = json.load(f)\n","\n","    token_to_index = mappings['token_to_index']\n","    vocab_size = len(token_to_index) + 1 \n","#     model = BiLM(hidden_dim=128, num_layers=2, vocab_size=vocab_size)\n","    model = ELMoLanguageModel(vocab_size, 300, 150).to(device)\n","    \n","    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","    model.eval()    \n","    \n","    return model, token_to_index\n","\n","\n","model_path = '/kaggle/input/final-elmo-model/bilm_marathi_model (1).pth'\n","mappings_path = '/kaggle/input/final-elmo-model/marathi_mappings.json'\n","\n","model, token_to_index = load_model_and_mappings(model_path, mappings_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:29:32.690909Z","iopub.status.busy":"2024-05-08T17:29:32.690296Z","iopub.status.idle":"2024-05-08T17:29:32.706196Z","shell.execute_reply":"2024-05-08T17:29:32.705322Z","shell.execute_reply.started":"2024-05-08T17:29:32.690875Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","from indicnlp.tokenize import indic_tokenize\n","\n","def preprocess_text(text, language='mr'):\n","    \"\"\"\n","    Apply preprocessing steps to the given text.\n","    \"\"\"\n","    text = remove_non_textual_elements(text)\n","    text = normalize_quotation_marks(text)\n","    text = ensure_utf8_encoding(text)\n","    sentences = tokenize_sentences(text)\n","    sentences_SOS = [\"<SOS> \"+sentence+\" <EOS>\" for sentence in sentences]\n","    tokenized_sentences = [tokenize_words_indicnlp(sentence, language) for sentence in sentences_SOS]\n","    return ' '.join([' '.join(sentence) for sentence in tokenized_sentences])\n","\n","def remove_non_textual_elements(text):\n","    text = re.sub(r'<[^>]+>', '', text)\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","def normalize_quotation_marks(text):\n","    text = text.replace('“', '\"').replace('”', '\"')\n","    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n","    return text\n","\n","def ensure_utf8_encoding(text):\n","    return text.encode('utf-8', errors='ignore').decode('utf-8')\n","\n","def tokenize_sentences(text):\n","    sentences = re.split(r'[।\\n\\.]+', text)\n","    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n","    return sentences\n","\n","def tokenize_words_indicnlp(sentence, language='mr'):\n","    return indic_tokenize.trivial_tokenize(sentence, lang=language)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:29:32.707596Z","iopub.status.busy":"2024-05-08T17:29:32.707284Z","iopub.status.idle":"2024-05-08T17:30:13.799119Z","shell.execute_reply":"2024-05-08T17:30:13.798203Z","shell.execute_reply.started":"2024-05-08T17:29:32.707572Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]},{"name":"stdout","output_type":"stream","text":["Train Dataset:                                          sectionText correctTitle  \\\n","0  मालिकेतील आधल्या खेळांप्रमाणे \"सिव्हलिजेशन ५\" ...       titleB   \n","1  गावात एटीएम उपलब्ध आहे. \\nगावात व्यापारी बँक उ...       titleB   \n","2  मराठवाड्यातील किंबहुना महाराष्ट्रातील दिवाळी श...       titleA   \n","3  डॉ. बाबासाहेब आंबेडकरांनी समाजवादाचा पुरस्कार ...       titleB   \n","4  चीनमध्ये अनेकांचा असा समज आहे की वाघांचे काही ...       titleB   \n","\n","                       titleA                titleB  \\\n","0              सामाजिक नीत्या                  शहरे   \n","1             शैक्षणिक सुविधा    बाजार व पतव्यवस्था   \n","2  महाराष्ट्रातील लोकसंस्कृती              जैन धर्म   \n","3               चीनबाबत विचार  समाजवादासंबंधी विचार   \n","4           आहार व शिकारपद्धत             चिनी औषधे   \n","\n","                      titleC         titleD  \\\n","0                      खेळणे     घटक व लढाई   \n","1            संपर्क व दळणवळण         आरोग्य   \n","2       दीपावलीची विविध नावे         भाऊबीज   \n","3  मूलभूत हक्कांसंबंधी विचार  धार्मिक विचार   \n","4            वाघ-मानव संघर्ष          रशिया   \n","\n","                                          url  \n","0  https://mr.wikipedia.org/wiki?curid=118187  \n","1    https://mr.wikipedia.org/wiki?curid=3488  \n","2    https://mr.wikipedia.org/wiki?curid=2465  \n","3  https://mr.wikipedia.org/wiki?curid=215324  \n","4    https://mr.wikipedia.org/wiki?curid=9604  \n","Test Dataset:                                          sectionText correctTitle  \\\n","0  नंगा पर्वत हे हिमालयाचे पश्चिम टोक मानले जाते....       titleB   \n","1  जगाच्या नकाशावर बोईसरचे अक्षांश १९.८° उत्तर, आ...       titleC   \n","2  सुरुवातीच्या काळात क्लबला लॅकॅशीयर आणि यॅर्कशी...       titleB   \n","3  भारत सरकारने नागरी उड्डाण क्षेत्रामध्ये परकीय ...       titleB   \n","4  हे मध्यम आकाराचे लहान गाव आहे.२०११ च्या भारतीय...       titleA   \n","\n","          titleA                         titleB        titleC  \\\n","0      वैशिष्ट्य                          भूगोल   यशस्वी चढाई   \n","1         शिक्षण                         संस्था  भौगोलिक सीमा   \n","2  क्लबचा इतिहास  मालकी हक्क आणि वित्त व्यवस्था     विजेतेपदे   \n","3      उपकंपन्या                     व्हिस्टारा  गंतव्यस्थाने   \n","4        लोकजीवन                         हवामान        संदर्भ   \n","\n","                titleD                                         url  \n","0  सु‍रूवातीच्या चढाया   https://mr.wikipedia.org/wiki?curid=18399  \n","1       औद्योगिक वसाहत  https://mr.wikipedia.org/wiki?curid=102770  \n","2               खेळाडू   https://mr.wikipedia.org/wiki?curid=15524  \n","3                 ताफा   https://mr.wikipedia.org/wiki?curid=78022  \n","4         नागरी सुविधा  https://mr.wikipedia.org/wiki?curid=247923  \n","Validation Dataset:                                          sectionText correctTitle  \\\n","0  अलेक्झांडरच्या म्रुत्यूनंतर त्याचे साम्राज्य त...       titleA   \n","1  प्रशिक्षणाथींची खिल्ली उडवणे, त्याच्याशी उद्धट...       titleC   \n","2  The consonants that are employed for voiced as...       titleB   \n","3  मराठी भाषेत र लागून बनलेल्या जोडाक्षरांना रफार...       titleB   \n","4  दाभोळ परिसरातील देर्दे हद्दीत (बंदरातून नदीच्य...       titleC   \n","\n","                       titleA                 titleB  \\\n","0          ग्रीक-कुशाण राज्ये     युरोपीयन वसाहत युग   \n","1            नियमांचे उल्लंघन              उपाययोजना   \n","2               Vowels - स्वर       Voiced aspirates   \n","3             'र'फार प्रकार ५              जोडाक्षरे   \n","4  मक्केच्या प्रवासाठीचे बंदर  दाभोळमदील काही मंदिरे   \n","\n","                         titleC                          titleD  \\\n","0        उत्तर मध्ययुगीन राज्ये            शुंग, शक आणि सातवाहन   \n","1  रॅगिंगमध्ये येणार्‍या गोष्टी  रॅगिंग प्रतिबंधाबाबत कार्यवाही   \n","2                   Other Signs                  Sounds - ध्वनी   \n","3               'र'फार प्रकार ३                 ’र’फार प्रकार १   \n","4       थडगी, पीर, दर्गे, मशिदी                 दालभेश्वर मंदिर   \n","\n","                                          url  \n","0    https://mr.wikipedia.org/wiki?curid=3334  \n","1  https://mr.wikipedia.org/wiki?curid=179106  \n","2   https://mr.wikipedia.org/wiki?curid=12577  \n","3  https://mr.wikipedia.org/wiki?curid=143652  \n","4   https://mr.wikipedia.org/wiki?curid=71979  \n"]}],"source":["import pandas as pd\n","import fasttext\n","import fasttext.util\n","\n","ft_model = fasttext.load_model('/kaggle/input/pre-trained-model-indicft/indicnlp.ft.mr.300.bin')\n","\n","def load_dataset(parquet_path):\n","    \"\"\"Load dataset from a Parquet file.\"\"\"\n","    return pd.read_parquet(parquet_path)\n","\n","train_path = '/kaggle/input/wstp-mr/train-00000-of-00001.parquet'\n","test_path = '/kaggle/input/wstp-mr/test-00000-of-00001.parquet'\n","val_path = '/kaggle/input/wstp-mr/validation-00000-of-00001.parquet'\n","\n","train_df = load_dataset(train_path)\n","test_df = load_dataset(test_path)\n","val_df = load_dataset(val_path)\n","\n","print(\"Train Dataset:\", train_df.head())\n","print(\"Test Dataset:\", test_df.head())\n","print(\"Validation Dataset:\", val_df.head())"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:30:13.800999Z","iopub.status.busy":"2024-05-08T17:30:13.800475Z","iopub.status.idle":"2024-05-08T17:30:22.126363Z","shell.execute_reply":"2024-05-08T17:30:22.125481Z","shell.execute_reply.started":"2024-05-08T17:30:13.800966Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                         sectionText correctTitle  \\\n","0  < SOS > मालिकेतील आधल्या खेळांप्रमाणे \" सिव्हल...       titleB   \n","1  < SOS > गावात एटीएम उपलब्ध आहे < EOS > < SOS >...       titleB   \n","2  < SOS > मराठवाड्यातील किंबहुना महाराष्ट्रातील ...       titleA   \n","3  < SOS > डॉ < EOS > < SOS > बाबासाहेब आंबेडकरां...       titleB   \n","4  < SOS > चीनमध्ये अनेकांचा असा समज आहे की वाघां...       titleB   \n","\n","                       titleA                titleB  \\\n","0              सामाजिक नीत्या                  शहरे   \n","1             शैक्षणिक सुविधा    बाजार व पतव्यवस्था   \n","2  महाराष्ट्रातील लोकसंस्कृती              जैन धर्म   \n","3               चीनबाबत विचार  समाजवादासंबंधी विचार   \n","4           आहार व शिकारपद्धत             चिनी औषधे   \n","\n","                      titleC         titleD  \\\n","0                      खेळणे     घटक व लढाई   \n","1            संपर्क व दळणवळण         आरोग्य   \n","2       दीपावलीची विविध नावे         भाऊबीज   \n","3  मूलभूत हक्कांसंबंधी विचार  धार्मिक विचार   \n","4            वाघ-मानव संघर्ष          रशिया   \n","\n","                                          url  \n","0  https://mr.wikipedia.org/wiki?curid=118187  \n","1    https://mr.wikipedia.org/wiki?curid=3488  \n","2    https://mr.wikipedia.org/wiki?curid=2465  \n","3  https://mr.wikipedia.org/wiki?curid=215324  \n","4    https://mr.wikipedia.org/wiki?curid=9604  \n"]}],"source":["def preprocess_dataset(df, text_column='text'):\n","    df[text_column] = df[text_column].apply(lambda x: preprocess_text(x))\n","    return df\n","\n","train_df_preprocessed = preprocess_dataset(train_df, 'sectionText')\n","test_df_preprocessed = preprocess_dataset(test_df, 'sectionText')\n","val_df_preprocessed = preprocess_dataset(val_df, 'sectionText')\n","\n","print(train_df_preprocessed.head())"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Baseline"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T17:30:22.128295Z","iopub.status.busy":"2024-05-08T17:30:22.127922Z","iopub.status.idle":"2024-05-08T17:42:19.701190Z","shell.execute_reply":"2024-05-08T17:42:19.698301Z","shell.execute_reply.started":"2024-05-08T17:30:22.128262Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240508_173055-28eoy40r</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/aryan-verma/ELMO_FOR_INDIAN_LANGUAGES_GROUP-30/runs/28eoy40r' target=\"_blank\">Downtask2-Train_elmo_Marathi</a></strong> to <a href='https://wandb.ai/aryan-verma/ELMO_FOR_INDIAN_LANGUAGES_GROUP-30' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/aryan-verma/ELMO_FOR_INDIAN_LANGUAGES_GROUP-30' target=\"_blank\">https://wandb.ai/aryan-verma/ELMO_FOR_INDIAN_LANGUAGES_GROUP-30</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/aryan-verma/ELMO_FOR_INDIAN_LANGUAGES_GROUP-30/runs/28eoy40r' target=\"_blank\">https://wandb.ai/aryan-verma/ELMO_FOR_INDIAN_LANGUAGES_GROUP-30/runs/28eoy40r</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1/10: 100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Average Loss: 1.3447618622605393\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/10: 100%|██████████| 164/164 [01:51<00:00,  1.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Average Loss: 1.3080241251282576\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/10: 100%|██████████| 164/164 [01:49<00:00,  1.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Average Loss: 1.2907281895963156\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/10: 100%|██████████| 164/164 [01:51<00:00,  1.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: Average Loss: 1.2787548296335267\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/10: 100%|██████████| 164/164 [01:51<00:00,  1.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Average Loss: 1.2640570961847537\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 164/164 [01:50<00:00,  1.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.38      0.38      0.38      2623\n","           1       0.37      0.37      0.37      2658\n","           2       0.38      0.37      0.37      2560\n","           3       0.37      0.37      0.37      2605\n","\n","    accuracy                           0.37     10446\n","   macro avg       0.37      0.37      0.37     10446\n","weighted avg       0.37      0.37      0.37     10446\n","\n"]},{"ename":"NameError","evalue":"name 'confusion_matrix' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m predicted_labels_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(train_embeddings, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(train_labels, predicted_labels_train))\n\u001b[0;32m--> 136\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m(train_labels, predicted_labels_train)\n\u001b[1;32m    137\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m)) \n\u001b[1;32m    138\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(font_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.4\u001b[39m)  \n","\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"]}],"source":["from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import classification_report\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","import numpy as np\n","import wandb\n","\n","class TitleSelectionDataset(Dataset):\n","    def __init__(self, df, ft_model, tokenizer, lang='mr'):\n","        self.df = df\n","        self.ft_model = ft_model\n","        self.tokenizer = tokenizer\n","        self.lang = lang\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        section_text = self.text_to_embedding(row['sectionText'])\n","        titles = [\n","            self.text_to_embedding(row['titleA']),\n","            self.text_to_embedding(row['titleB']),\n","            self.text_to_embedding(row['titleC']),\n","            self.text_to_embedding(row['titleD'])\n","        ]\n","\n","        title_keys = ['titleA', 'titleB', 'titleC', 'titleD']\n","        correct_title_key = row['correctTitle']  \n","        label = title_keys.index(correct_title_key) \n","\n","        return section_text, titles, label\n","\n","    def text_to_embedding(self, text):\n","        tokens = self.tokenizer(text, lang=self.lang)\n","        embeddings = [self.ft_model.get_word_vector(token) for token in tokens]\n","        embeddings_array = np.array(embeddings) \n","        return torch.tensor(embeddings_array, dtype=torch.float) \n","\n","def collate_fn(batch):\n","    section_texts, titles, labels = zip(*batch)\n","    section_texts_padded = pad_sequence(section_texts, batch_first=True, padding_value=0.0)\n","    titles_padded = [pad_sequence([t[i] for t in titles], batch_first=True, padding_value=0.0) for i in range(4)]\n","    labels = torch.tensor(labels, dtype=torch.long)\n","    return section_texts_padded, titles_padded, labels\n","\n","class TitleClassifier(nn.Module):\n","    def __init__(self, elmo_model, hidden_dim):\n","        super(TitleClassifier, self).__init__()\n","        self.elmo_model = elmo_model\n","        self.classifier = nn.Sequential(\n","            nn.Linear(hidden_dim * 2 * 2, hidden_dim),  \n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","    def forward(self, section_text, titles):\n","        _, _, section_embeddings = self.elmo_model(section_text)\n","        section_embeddings = section_embeddings.mean(dim=1)\n","\n","        scores = []\n","        for title in titles:\n","            _, _, title_embeddings = self.elmo_model(title)\n","            title_embeddings = title_embeddings.mean(dim=1)\n","            combined_embeddings = torch.cat((section_embeddings, title_embeddings), dim=1)\n","            score = self.classifier(combined_embeddings)  \n","            scores.append(score.squeeze())\n","        \n","        scores = torch.stack(scores, dim=1).squeeze(-1) \n","        return scores\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","hidden_dim = 150\n","vocab_size = len(token_to_index)  \n","elmo_model = ELMoLanguageModel(vocab_size, 300, hidden_dim).to(device) \n","classifier = TitleClassifier(elmo_model, hidden_dim).to(device) \n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001) \n","\n","train_loader = DataLoader(TitleSelectionDataset(train_df_preprocessed, ft_model, indic_tokenize.trivial_tokenize), batch_size=64, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(TitleSelectionDataset(test_df_preprocessed, ft_model, indic_tokenize.trivial_tokenize), batch_size=64, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(TitleSelectionDataset(val_df_preprocessed, ft_model, indic_tokenize.trivial_tokenize), batch_size=64, shuffle=True, collate_fn=collate_fn)\n","\n","\n","wandb.init(project='ELMO_FOR_INDIAN_LANGUAGES_GROUP-30', name=\"Downtask2-Train_elmo_Marathi\")\n","\n","for epoch in range(5):\n","    classifier.train()\n","    total_loss = 0\n","    for section_texts, candidate_titles, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n","        section_texts = section_texts.to(device)\n","        candidate_titles = [title.to(device) for title in candidate_titles]\n","        labels = labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        outputs = classifier(section_texts, candidate_titles)\n","#         print(outputs.shape)\n","#         print(labels.shape)\n","        loss = criterion(outputs, labels)  \n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","    wandb.log({\"train_loss\": total_loss / len(train_loader)})\n","    print(f\"Epoch {epoch+1}: Average Loss: {total_loss / len(train_loader)}\")\n","\n","def extract_features_elmo(dataloader, classifier, device):\n","    classifier.eval()  \n","    all_embeddings = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for section_texts, candidate_titles, labels in tqdm(dataloader):\n","            section_texts = section_texts.to(device)\n","            candidate_titles = [title.to(device) for title in candidate_titles]\n","            outputs = classifier(section_texts, candidate_titles)\n","            all_embeddings.extend(outputs.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    return np.array(all_embeddings), np.array(all_labels)\n","\n","# =============================================================================================\n","\n","train_embeddings, train_labels = extract_features_elmo(train_loader, classifier, device)\n","predicted_labels_train = np.argmax(train_embeddings, axis=1)\n","\n","print(classification_report(train_labels, predicted_labels_train))\n","cm = confusion_matrix(train_labels, predicted_labels_train)\n","plt.figure(figsize=(10, 7)) \n","sns.set(font_scale=1.4)  \n","sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap=plt.cm.Blues) \n","\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()\n","\n","# =============================================================================================\n","test_embeddings, test_labels = extract_features(test_loader, classifier, device)\n","predicted_labels = np.argmax(test_embeddings, axis=1)\n","\n","print(classification_report(test_labels, predicted_labels))\n","cm = confusion_matrix(test_labels, predicted_labels)\n","plt.figure(figsize=(10, 7)) \n","sns.set(font_scale=1.4)  \n","sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap=plt.cm.Blues)  \n","\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()\n","\n","test_embeddings, test_labels = extract_features(test_loader, classifier_baseline, device)\n","predicted_labels = np.argmax(test_embeddings, axis=1)\n","\n","print(classification_report(test_labels, predicted_labels))\n","cm = confusion_matrix(test_labels, predicted_labels)\n","plt.figure(figsize=(10, 7)) \n","sns.set(font_scale=1.4)  \n","sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap=plt.cm.Blues)  \n","\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### 1.2 ELMO"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:42:19.703608Z","iopub.status.idle":"2024-05-08T17:42:19.703999Z","shell.execute_reply":"2024-05-08T17:42:19.703840Z","shell.execute_reply.started":"2024-05-08T17:42:19.703825Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import numpy as np\n","import wandb\n","\n","class TitleSelectionDataset(Dataset):\n","    def __init__(self, df, ft_model, tokenizer, lang='mr'):\n","        self.df = df\n","        self.ft_model = ft_model\n","        self.tokenizer = tokenizer\n","        self.lang = lang\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        section_text = self.text_to_embedding(row['sectionText'])\n","        titles = [\n","            self.text_to_embedding(row['titleA']),\n","            self.text_to_embedding(row['titleB']),\n","            self.text_to_embedding(row['titleC']),\n","            self.text_to_embedding(row['titleD'])\n","        ]\n","\n","        title_keys = ['titleA', 'titleB', 'titleC', 'titleD']\n","        correct_title_key = row['correctTitle']  \n","        label = title_keys.index(correct_title_key) \n","\n","        return section_text, titles, label\n","\n","    def text_to_embedding(self, text):\n","        tokens = self.tokenizer(text, lang=self.lang)\n","        embeddings = [self.ft_model.get_word_vector(token) for token in tokens]\n","        embeddings_array = np.array(embeddings) \n","        return torch.tensor(embeddings_array, dtype=torch.float)\n","\n","def collate_fn(batch):\n","    section_texts, titles, labels = zip(*batch)\n","    section_texts_padded = pad_sequence(section_texts, batch_first=True, padding_value=0.0)\n","    titles_padded = [pad_sequence([t[i] for t in titles], batch_first=True, padding_value=0.0) for i in range(4)]\n","    labels = torch.tensor(labels, dtype=torch.long)\n","    return section_texts_padded, titles_padded, labels\n","\n","class TitleClassifier(nn.Module):\n","    def __init__(self, input_dim):\n","        super(TitleClassifier, self).__init__()\n","        self.classifier = nn.Sequential(\n","            nn.Linear(input_dim * 2, input_dim),  \n","            nn.ReLU(),\n","            nn.Linear(input_dim, 1)\n","        )\n","\n","    def forward(self, section_text, titles):\n","        section_embeddings = section_text.mean(dim=1) \n","\n","        scores = []\n","        for title in titles:\n","            title_embeddings = title.mean(dim=1)  \n","            combined_embeddings = torch.cat((section_embeddings, title_embeddings), dim=1)\n","            score = self.classifier(combined_embeddings)  \n","            scores.append(score.squeeze())\n","\n","        scores = torch.stack(scores, dim=1).squeeze(-1)\n","        return scores\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","input_dim = 300  \n","classifier_baseline = TitleClassifier(input_dim).to(device) \n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer_baseline = torch.optim.Adam(classifier_baseline.parameters(), lr=0.001)\n","\n","train_loader = DataLoader(TitleSelectionDataset(train_df_preprocessed, ft_model, indic_tokenize.trivial_tokenize), batch_size=64, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(TitleSelectionDataset(test_df_preprocessed, ft_model, indic_tokenize.trivial_tokenize), batch_size=64, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(TitleSelectionDataset(val_df_preprocessed, ft_model, indic_tokenize.trivial_tokenize), batch_size=64, shuffle=True, collate_fn=collate_fn)\n","\n","wandb.init(project='ELMO_FOR_INDIAN_LANGUAGES_GROUP-30', name=\"Downtask2-Train_baseline_Marathi\")\n","\n","for epoch in range(5):\n","    classifier_baseline.train()\n","    total_loss = 0\n","    for section_texts, candidate_titles, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n","        section_texts = section_texts.to(device)\n","        candidate_titles = [title.to(device) for title in candidate_titles]\n","        labels = labels.to(device)\n","        \n","        optimizer_baseline.zero_grad()\n","        outputs = classifier_baseline(section_texts, candidate_titles)\n","        loss = criterion(outputs, labels)  \n","        loss.backward()\n","        optimizer_baseline.step()\n","        \n","        total_loss += loss.item()\n","    wandb.log({\"train_loss\": total_loss / len(train_loader)})\n","    print(f\"Epoch {epoch+1}: Average Loss: {total_loss / len(train_loader)}\")\n","\n","def extract_features(dataloader, classifier, device):\n","    classifier.eval()  \n","    all_embeddings = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for section_texts, candidate_titles, labels in tqdm(dataloader):\n","            section_texts = section_texts.to(device)\n","            candidate_titles = [title.to(device) for title in candidate_titles]\n","            outputs = classifier(section_texts, candidate_titles)\n","            all_embeddings.extend(outputs.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    return np.array(all_embeddings), np.array(all_labels)\n","\n","train_embeddings, train_labels = extract_features(train_loader, classifier_baseline, device)\n","predicted_labels_train = np.argmax(train_embeddings, axis=1)\n","\n","print(classification_report(train_labels, predicted_labels_train))\n","cm = confusion_matrix(train_labels, predicted_labels_train)\n","plt.figure(figsize=(10, 7)) \n","sns.set(font_scale=1.4)  \n","sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap=plt.cm.Blues) \n","\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4759390,"sourceId":8066925,"sourceType":"datasetVersion"},{"datasetId":4764681,"sourceId":8074184,"sourceType":"datasetVersion"},{"datasetId":4794710,"sourceId":8115633,"sourceType":"datasetVersion"},{"datasetId":4794857,"sourceId":8115824,"sourceType":"datasetVersion"},{"datasetId":4951206,"sourceId":8341329,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
